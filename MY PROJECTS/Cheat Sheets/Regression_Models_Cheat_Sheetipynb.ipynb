
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# REGRESSION MODELS CHEAT SHEET\n",
        "This notebook contains:\n",
        "- 10 most common regression models\n",
        "- When to use them\n",
        "- Their syntaxes (for importing, fitting, testing)"
      ],
      "metadata": {
        "id": "KHQu82BPbGEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For the sake of this cheat sheet\n",
        "We'll load some data into the notebook to see the models in action\n",
        "\n"
      ],
      "metadata": {
        "id": "zMIFtLL3cGx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "df_test = pd.read_csv('/content/sample_data/california_housing_test.csv')\n",
        "\n",
        "df_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUzFF5T3cGSM",
        "outputId": "75631f2f-99fc-44f4-ef02-c295b94e9859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
              "       'total_bedrooms', 'population', 'households', 'median_income',\n",
              "       'median_house_value'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train[['housing_median_age', 'total_rooms','total_bedrooms', 'population', 'households', 'median_income']]\n",
        "y_train = df_train['median_house_value']\n",
        "\n",
        "X_test = df_test[['housing_median_age', 'total_rooms','total_bedrooms', 'population', 'households', 'median_income']]\n",
        "y_test = df_test['median_house_value']"
      ],
      "metadata": {
        "id": "oPgvmwIYdBto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Linear Regression**\n",
        "\n",
        "To be used:\n",
        "- When the relationship between the features and the target variable is linear.\n",
        "- When you need a simple and interpretable model for regression tasks.\n"
      ],
      "metadata": {
        "id": "RTdRWQEEbjaS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJg3nZAkbCGN",
        "outputId": "00637408-ce68-479f-d5c5-61676a04f8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5808966246.7101965 0.5458835346746116\n"
          ]
        }
      ],
      "source": [
        "# Importing\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Fitting\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(mse, r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "To be used:\n",
        "- When dealing with multicollinearity in the data or preventing overfitting\n",
        "- When you have a large number of features."
      ],
      "metadata": {
        "id": "YJH-WU1KgPYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Fitting\n",
        "model = Ridge(alpha=1.0)  # alpha is the regularization strength\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(mse, r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnn50UkbeuHy",
        "outputId": "f4fe6856-56d9-4dee-f427-38eb7806b128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5808960076.618068 0.5458840170221257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Lasso Regression (L1 Regularization)**\n",
        "To be used:\n",
        "- When you want a sparse model with some feature coefficients exactly equal to zero.\n",
        "- Useful for feature selection and when dealing with high-dimensional data."
      ],
      "metadata": {
        "id": "Qw3BQMC3g5mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Fitting\n",
        "model = Lasso(alpha=1.0)  # alpha is the regularization strength\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(mse, r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtcUR1NchVGo",
        "outputId": "0047011a-216e-44c2-acb6-3d3b1b3feb7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5808964096.664563 0.5458837027546248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4. ElasticNet Regression**\n",
        "To be used:\n",
        "- When you want a combination of L1 and L2 regularization (both Ridge and Lasso benefits).\n",
        "- Useful for dealing with datasets with multicollinearity and a large number of features."
      ],
      "metadata": {
        "id": "Z_P4PfqohV5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Fitting\n",
        "model = ElasticNet(alpha=1.0, l1_ratio=0.5)  # alpha is the regularization strength, l1_ratio controls the balance between L1 and L2 regularization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(mse, r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM4NbmaGhh6v",
        "outputId": "b85f55b3-d2a7-4d5a-ac66-f69e48f33683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5916733328.35446 0.5374588332533492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Decision Tree Regression**\n",
        "To be used:\n",
        "- When the relationship between features and the target variable is non-linear and can be represented as a hierarchical decision tree."
      ],
      "metadata": {
        "id": "UvMCzyYVmaiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Fitting\n",
        "model = DecisionTreeRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(mse, r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYC0Gk5Omm8F",
        "outputId": "849ed1c5-a7ae-44d5-e469-66771018b61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9280312694.58 0.27451070999702964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**6. Random Forest Regression**\n",
        "To be used:\n",
        "- When you need better predictive accuracy and reduced overfitting compared to a single Decision Tree."
      ],
      "metadata": {
        "id": "IdUKjqIDnSh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Fitting\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(mse, r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4c9tDKDmvyJ",
        "outputId": "fe2b3ab2-4d5e-44b3-9f85-a8ffb05a325f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4602571636.797053 0.6401935431639223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7. Gradient Boosting Regression**\n",
        "To be used:\n",
        "- When high predictive accuracy is required, and complex relationships exist in the data."
      ],
      "metadata": {
        "id": "qPVb4-duntYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Fitting\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(mse, r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxzrPeHonfqn",
        "outputId": "f6442644-559d-4cd3-c68b-f67e153355d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4735855249.383914 0.6297740846125799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**8. Support Vector Regression (SVR)**\n",
        "To be used:\n",
        "- When you have non-linear data and need a powerful model capable of handling high-dimensional data.\n"
      ],
      "metadata": {
        "id": "-04vwhQLoGsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Fitting\n",
        "model = SVR(kernel='rbf')  # rbf is a popular kernel for non-linear data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(mse, r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8LTE76koHDh",
        "outputId": "0e444e6d-43b6-4366-c829-678effaef152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13427202411.670599 -0.04967277126969005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**9. K-Nearest Neighbors (KNN) Regression**\n",
        "To be used:\n",
        "- When local patterns are important and you want a simple non-linear regression approach."
      ],
      "metadata": {
        "id": "85rK6licoXuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Fitting\n",
        "model = KNeighborsRegressor(n_neighbors=5)  # n_neighbors is the number of neighbors to consider\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(mse, r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXD9TyyFodmo",
        "outputId": "b44ef236-ea37-486f-aced-e87f11cbc05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9544759257.744932 0.25383757583999844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MORE CASE STUDY\n",
        "### Scenarios in which each regression model is most suitable:\n",
        "\n",
        "1. **Linear Regression**:\n",
        "   - Scenario: When the relationship between the input features and the target variable is approximately linear.\n",
        "   - Use Case: Predicting housing prices based on features like area, number of rooms, etc., where the relationship is expected to be linear.\n",
        "\n",
        "2. **Ridge Regression (L2 Regularization)**:\n",
        "   - Scenario: When dealing with multicollinearity (high correlation between predictor variables) in the data, or to prevent overfitting when there are many features.\n",
        "   - Use Case: Predicting stock prices using multiple technical indicators that might be highly correlated.\n",
        "\n",
        "3. **Lasso Regression (L1 Regularization)**:\n",
        "   - Scenario: When you have a high-dimensional dataset with many features, and you want to perform feature selection to identify the most relevant ones.\n",
        "   - Use Case: Analyzing gene expression data to identify the most important genes in predicting a disease outcome.\n",
        "\n",
        "4. **ElasticNet Regression**:\n",
        "   - Scenario: When you want to balance the benefits of both L1 and L2 regularization and handle datasets with multicollinearity and a large number of features.\n",
        "   - Use Case: Predicting customer churn in a telecommunications company where there are both highly correlated features and many potential predictor variables.\n",
        "\n",
        "5. **Decision Tree Regression**:\n",
        "   - Scenario: When the relationship between features and the target variable is non-linear and can be represented as a hierarchy of decisions.\n",
        "   - Use Case: Predicting the price of a used car based on attributes such as age, mileage, and make, where the relationships might be non-linear.\n",
        "\n",
        "6. **Random Forest Regression**:\n",
        "   - Scenario: When you need better predictive accuracy and want to reduce overfitting compared to a single Decision Tree.\n",
        "   - Use Case: Predicting the demand for a product based on multiple factors, where there could be complex interactions between the predictors.\n",
        "\n",
        "7. **Gradient Boosting Regression**:\n",
        "   - Scenario: When high predictive accuracy is crucial, and there are complex relationships in the data.\n",
        "   - Use Case: Predicting the risk of credit default based on historical financial data and other relevant factors.\n",
        "\n",
        "8. **Support Vector Regression (SVR)**:\n",
        "   - Scenario: When dealing with non-linear data and you need a powerful model capable of handling high-dimensional data.\n",
        "   - Use Case: Predicting the price of a house based on features like location, square footage, and amenities, where there might not be a linear relationship.\n",
        "\n",
        "9. **K-Nearest Neighbors (KNN) Regression**:\n",
        "   - Scenario: When local patterns are important, and you want a simple non-linear regression approach.\n",
        "   - Use Case: Predicting daily temperatures based on historical weather data from neighboring locations.\n"
      ],
      "metadata": {
        "id": "3VT_MZzNqN6u"
      }
    }
  ]
}